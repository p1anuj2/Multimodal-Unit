# Multimodal-Unit
Official implementation of Multi-Modal UNIT (MMU) â€” a unified transformer for joint languageâ€“vision understanding and content generation.
# ğŸ§  Multi-Modal UNIT (MMU)

Official implementation of the paper:  
**"Unified Transformer Framework for Integrated Languageâ€“Vision Understanding and Content Generation"**  
Submitted to *The Visual Computer* (Springer, 2025)

---

## ğŸ” Overview
**Multi-Modal UNIT (MMU)** is a unified transformer architecture designed to bridge visual and linguistic understanding within a **single-stream framework**.  
Unlike traditional dual-encoder approaches that treat text and image separately, MMU introduces **lightweight modality adapters** that project visual and textual embeddings into a shared representational space.  
Through **shared attention layers** and a **hybrid optimization strategy** combining contrastive and generative objectives, MMU achieves both strong accuracy and high computational efficiency across multimodal tasks.

---
## ğŸ§  Model Architecture
<p align="center">
  <img src="architecture.png" width="700">
</p>

*Figure 1. The overall architecture of Multi-Modal UNIT (MMU).*



## ğŸ§© Key Contributions
- âœ… **Unified single-stream transformer** for both language and vision processing.  
- ğŸ§  **Lightweight modality adapters** for efficient feature alignment.  
- ğŸ”„ **Hybrid objectives** integrating contrastive (understanding) and generative (captioning/reasoning) training.  
- ğŸ“Š **Consistent performance** across COCO, Flickr30k, VQAv2, and NLVR2 benchmarks.  
- âš¡ **210M parameters** with **70 ms per-sample inference latency**, achieving a strong accuracyâ€“efficiency balance.

---
## âš–ï¸ License & Copyright
This project is licensed under the **MIT License**.  
Â© 2025 **ANUJ ATTRI**. All rights reserved.  
When using this code or referencing results, please cite the associated paper published in *The Visual Computer (2025)*.
