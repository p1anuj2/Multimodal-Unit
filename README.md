# Multimodal-Unit
Official implementation of Multi-Modal UNIT (MMU) â€” a unified transformer for joint languageâ€“vision understanding and content generation.
# ğŸ§  Multi-Modal UNIT (MMU)

Official implementation of the paper:  
**"Unified Transformer Framework for Integrated Languageâ€“Vision Understanding and Content Generation"**  
Submitted to *The Visual Computer* (Springer, 2025)

---

## ğŸ” Overview
**Multi-Modal UNIT (MMU)** is a unified transformer architecture designed to bridge visual and linguistic understanding within a **single-stream framework**.  
Unlike traditional dual-encoder approaches that treat text and image separately, MMU introduces **lightweight modality adapters** that project visual and textual embeddings into a shared representational space.  
Through **shared attention layers** and a **hybrid optimization strategy** combining contrastive and generative objectives, MMU achieves both strong accuracy and high computational efficiency across multimodal tasks.

---

## ğŸ§© Key Contributions
- âœ… **Unified single-stream transformer** for both language and vision processing.  
- ğŸ§  **Lightweight modality adapters** for efficient feature alignment.  
- ğŸ”„ **Hybrid objectives** integrating contrastive (understanding) and generative (captioning/reasoning) training.  
- ğŸ“Š **Consistent performance** across COCO, Flickr30k, VQAv2, and NLVR2 benchmarks.  
- âš¡ **210M parameters** with **70 ms per-sample inference latency**, achieving a strong accuracyâ€“efficiency balance.

---

